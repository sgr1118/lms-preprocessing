#!/usr/bin/env python
# coding: utf-8

# In[2]:


import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

csv_file_path = os.getenv('HOME')+'/aiffel/data_preprocess/data/trade.csv'
trade = pd.read_csv(csv_file_path) 
trade = trade
trade.head()


# # 10-2. ê²°ì¸¡ì¹˜

# In[3]:


print('ì „ì²´ ë°ì´í„° ê±´ìˆ˜:', len(trade))


# In[13]:


#print('ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ê°œìˆ˜')
len(trade) - trade.count()
# print(trade.info())
#print(trade.isna().count())
# ê¸°íƒ€ ì‚¬í•­ì€ ì „ë¶€ ê²°ì¸¡ì¹˜


# In[14]:


# ê¸°íƒ€ì‚¬í•­ ì‚­ì œ
trade = trade.drop(columns = ['ê¸°íƒ€ì‚¬í•­'])
trade.info()


# In[17]:


trade[trade.isnull().any(axis=1)]


# In[18]:


trade.dropna(how='all', subset = ['ìˆ˜ì¶œê±´ìˆ˜', 'ìˆ˜ì¶œê¸ˆì•¡', 'ìˆ˜ì…ê±´ìˆ˜', 'ìˆ˜ì…ê¸ˆì•¡', 'ë¬´ì—­ìˆ˜ì§€'], inplace = True)
trade[trade.isnull().any(axis=1)]


# In[19]:


trade.loc[[188, 191, 194]]


# In[23]:


# í‰ê· ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸° (ìˆ˜ì¶œê¸ˆì•¡)
trade.loc[191, 'ìˆ˜ì¶œê¸ˆì•¡'] = (trade.loc[188, 'ìˆ˜ì¶œê¸ˆì•¡'] + trade.loc[194, 'ìˆ˜ì¶œê¸ˆì•¡']) / 2
trade.loc[[191]]


# In[24]:


# ê³„ì‚°ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
trade.loc[191, 'ë¬´ì—­ìˆ˜ì§€'] = (trade.loc[191, 'ìˆ˜ì¶œê¸ˆì•¡'] - trade.loc[191, 'ìˆ˜ì…ê¸ˆì•¡'])
trade.loc[[191]]


# In[28]:


# ì»¬ëŸ¼ì˜ í‰ê· ê°’ì„ êµ¬í•´ì„œ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
print(round(trade['ìˆ˜ì¶œê¸ˆì•¡'].mean(),1))
# 6580880.8
trade.loc[191, 'ìˆ˜ì¶œê¸ˆì•¡'] = 6580880.8
trade.loc[[191]]


# # 10-3. ì¤‘ë³µëœ ë°ì´í„°

# In[32]:


# ì¤‘ë³µ í™•ì¸
# trade[trade.duplicated()] index 186, 187 ì¤‘ë³µ
trade[(trade['ê¸°ê°„'] == '2020ë…„ 03ì›”')&(trade['êµ­ê°€ëª…'] == 'ì¤‘êµ­')]


# In[34]:


# ì¤‘ë³µ ë°ì´í„° ì œê±°
trade.drop_duplicates(inplace=True)


# In[36]:


# DataFrame.drop_duplicates ìì„¸íˆ ì•Œì•„ë³´ê¸°
df = pd.DataFrame({'id':['001', '002', '003', '004', '002'], 
                   'name':['Park Yun', 'Kim Sung', 'Park Jin', 'Lee Han', 'Kim Min']})
df


# In[39]:


df.drop_duplicates(subset=['id'], keep='last')
df['id'].sort_values(ascending=True)


# # 10-4. ì´ìƒì¹˜ 

# In[40]:


# z-score method
def outlier(df, col, z):
    return df[abs(df[col] - np.mean(df[col]))/np.std(df[col])>z].index


# In[45]:


# z-score method ê³„ì‚°
trade.loc[outlier(trade, 'ë¬´ì—­ìˆ˜ì§€', 3)].count()


# In[47]:


# ì´ìƒì¹˜ ì•„ë‹Œ ê°’ë§Œ ì¶œë ¥í•˜ê¸°
def not_outlier(df, col, z):
    return df[abs(df[col] - np.mean(df[col]))/np.std(df[col]) <= z].index


# In[48]:


trade.loc[not_outlier(trade, 'ë¬´ì—­ìˆ˜ì§€', 1.5)]


# In[49]:


# IQR method (ì‚¬ë¶„ìœ„ ë²”ìœ„ìˆ˜ í™œìš©)

np.random.seed(2020)
data = np.random.randn(100)  # í‰ê·  0, í‘œì¤€í¸ì°¨ 1ì˜ ë¶„í¬ì—ì„œ 100ê°œì˜ ìˆ«ìë¥¼ ìƒ˜í”Œë§í•œ ë°ì´í„° ìƒì„±
data = np.concatenate((data, np.array([8, 10, -3, -5])))      # [8, 10, -3, -5])ë¥¼ ë°ì´í„° ë’¤ì— ì¶”ê°€í•¨
data


# In[50]:


fig, ax = plt.subplots()
ax.boxplot(data)
plt.show()


# In[54]:


# IQR method  ê³„ì‚°ì‹
# IQR = Q3 - Q1 (3ë¶„ìœ„ìˆ˜ - 1ë¶„ìœ„ìˆ˜)
# Q3, Q1 = np.percentile(data, [75, 25])
#print(Q3, Q1) # 0.5166477538712722 -0.6478448291078243
IQR = Q3 - Q1
IQR # 1.1644925829790964


# In[55]:


data[(Q1-1.5*IQR > data)|(Q3+1.5*IQR < data)]


# In[57]:


# ì´ìƒì¹˜ ì‹¤ìŠµ
def outlier2(df, col):
    return df[col].quantile(0.75) - df[col].quantile(0.25)

outlier2(trade, 'ë¬´ì—­ìˆ˜ì§€')


# # 10-5. ì •ê·œí™”

# In[67]:


# ì •ê·œë¶„í¬ë¥¼ ë”°ë¼ ëœë¤í•˜ê²Œ ë°ì´í„° xë¥¼ ìƒì„±í•©ë‹ˆë‹¤. 
x = pd.DataFrame({'A': np.random.randn(100)*4+4,
                 'B': np.random.randn(100)-1})
x


# In[69]:


# ë°ì´í„° xë¥¼ Standardization ê¸°ë²•ìœ¼ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤. 
x_stand = (x - x.mean())/x.std()
x_stand


# In[70]:


# ë°ì´í„° xë¥¼ min-max scaling ê¸°ë²•ìœ¼ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.
x_min_max = (x-x.min()) / (x.max() - x.min())
x_min_max


# In[72]:


# ìŠ¤ì¼€ì¼ë§ í›„ ë°ì´í„° ë¶„í˜¸ í™•ì¸
fig, axs = plt.subplots(1,2, figsize=(12, 4),
                        gridspec_kw={'width_ratios': [2, 1]})

axs[0].scatter(x['A'], x['B'])
axs[0].set_xlim(-5, 15)
axs[0].set_ylim(-5, 5)
axs[0].axvline(c='grey', lw=1)
axs[0].axhline(c='grey', lw=1)
axs[0].set_title('Original Data')

axs[1].scatter(x_stand['A'], x_stand['B'])
axs[1].set_xlim(-5, 5)
axs[1].set_ylim(-5, 5)
axs[1].axvline(c='grey', lw=1)
axs[1].axhline(c='grey', lw=1)
axs[1].set_title('Data after standardization')

plt.show()


# In[73]:


fig, axs = plt.subplots(1,2, figsize=(12, 4),
                        gridspec_kw={'width_ratios': [2, 1]})

axs[0].scatter(x['A'], x['B'])
axs[0].set_xlim(-5, 15)
axs[0].set_ylim(-5, 5)
axs[0].axvline(c='grey', lw=1)
axs[0].axhline(c='grey', lw=1)
axs[0].set_title('Original Data')

axs[1].scatter(x_min_max['A'], x_min_max['B'])
axs[1].set_xlim(-5, 5)
axs[1].set_ylim(-5, 5)
axs[1].axvline(c='grey', lw=1)
axs[1].axhline(c='grey', lw=1)
axs[1].set_title('Data after min-max scaling')

plt.show()


# In[75]:


# Standardization
# trade ë°ì´í„°ë¥¼ Standardization ê¸°ë²•ìœ¼ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤. 
# x_stand = (x - x.mean())/x.std()
cols = ['ìˆ˜ì¶œê±´ìˆ˜', 'ìˆ˜ì¶œê¸ˆì•¡', 'ìˆ˜ì…ê±´ìˆ˜', 'ìˆ˜ì…ê¸ˆì•¡', 'ë¬´ì—­ìˆ˜ì§€']
trade_Standard = (trade[cols]-trade[cols].mean()) / trade[cols].std()
# trade_Standard.head()
trade_Standard.describe()


# In[91]:


# trade ë°ì´í„°ë¥¼ min-max scaling ê¸°ë²•ìœ¼ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤. 
# x_min_max = (x-x.min()) / (x.max() - x.min())
trade[cols] = (trade[cols] - trade[cols].min()) / (trade[cols].max() - trade[cols].min())
trade.head()


# In[79]:


# sklearnì„ í™œìš©í•œ Scaler
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
train = [[10, -10], [30, 10], [50, 0]]
test = [[0, 1]]


# In[80]:


scaler.fit_transform(train)


# In[81]:


scaler.fit_transform(test)


# # 10-6 ì›-í•« ì¸ì½”ë”© (One-Hot Encoding)

# In[92]:


# #trade ë°ì´í„°ì˜ êµ­ê°€ëª… ì»¬ëŸ¼ ì›ë³¸
#print(trade['êµ­ê°€ëª…'].head())
# get_dummiesë¥¼ í†µí•´ êµ­ê°€ëª… ì›-í•« ì¸ì½”ë”©
country = pd.get_dummies(trade['êµ­ê°€ëª…'])
country.head()


# In[93]:


trade = pd.concat([trade, country], axis = 1)
trade.head()


# In[ ]:


# í•„ìš”ì—†


# In[97]:


trade2 = pd.read_csv(csv_file_path)
trade2.info()


# In[101]:


from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
trade2['êµ­ê°€ëª…'] = encoder.fit_transform(trade2['êµ­ê°€ëª…'])
trade2.head()


# In[ ]:





# # 10-7. êµ¬ê°„í™”

# In[118]:


salary = pd.Series([4300, 8370, 1750, 3830, 1840, 4220, 3020, 2290, 4740, 4600, 
                    2860, 3400, 4800, 4470, 2440, 4530, 4850, 4850, 4760, 4500, 
                    4640, 3000, 1880, 4880, 2240, 4750, 2750, 2810, 3100, 4290, 
                    1540, 2870, 1780, 4670, 4150, 2010, 3580, 1610, 2930, 4300, 
                    2740, 1680, 3490, 4350, 1680, 6420, 8740, 8980, 9080, 3990, 
                    4960, 3700, 9600, 9330, 5600, 4100, 1770, 8280, 3120, 1950, 
                    4210, 2020, 3820, 3170, 6330, 2570, 6940, 8610, 5060, 6370,
                    9080, 3760, 8060, 2500, 4660, 1770, 9220, 3380, 2490, 3450, 
                    1960, 7210, 5810, 9450, 8910, 3470, 7350, 8410, 7520, 9610, 
                    5150, 2630, 5610, 2750, 7050, 3350, 9450, 7140, 4170, 3090])
print("ğŸ‘½ Almost there..")


# In[103]:


# ë°ì´í„° êµ¬ê°„ í™•ì¸
salary.hist()


# In[104]:


# bins = [0, 2000, 4000, 6000, 8000, 10000]


# In[ ]:





# In[119]:


ctg = pd.cut(salary, bins=[0, 2000, 4000, 6000, 8000, 10000])
ctg


# In[108]:


print('salary[5]:', salary[5])
print('salary[5]ê°€ ì†í•œ ì¹´í…Œê³ ë¦¬:', ctg[0])


# In[109]:


ctg.value_counts().sort_index()


# In[110]:


ctg = pd.cut(salary, bins=6)
ctg


# In[111]:


ctg.value_counts().sort_index()


# In[112]:


ctg = pd.qcut(salary, q=5)
ctg


# In[113]:


print(ctg.value_counts().sort_index())


# # 10-7. ë°ì´í„° ì „ì²˜ë¦¬ ì´ ë³µìŠµ

# In[122]:


csv_file_path = os.getenv('HOME')+'/aiffel/data_preprocess/data/vgsales.csv'
vgsales = pd.read_csv(csv_file_path) 
vgsales_copy = vgsales.copy()
vgsales.head()


# In[123]:


vgsales_copy.info()


# In[124]:


# ê²°ì¸¡ì¹˜ í™•ì¸
vgsales_copy.isnull().sum() # year, Publisher


# In[127]:


vgsales_copy.dropna(how='any', subset=['Year', 'Publisher'], inplace = True)
vgsales_copy.isnull().sum()


# In[128]:


vgsales_copy.info()


# In[130]:


# ì¤‘ë³µëœ ë°ì´í„° í™•ì¸
vgsales_copy[vgsales_copy.duplicated()]
# ì¤‘ë³µ ë°ì´í„° ì—†ìŒ


# In[141]:


# ì´ìƒì¹˜ í™•ì¸ (IQRì‚¬ìš©)
def outlier2(df, col):
    return df[col].quantile(0.75) - df[col].quantile(0.25)

#outlier2(vgsales_copy, 'NA_Sales') 0.24
#outlier2(vgsales_copy, 'EU_Sales') 0.11
#outlier2(vgsales_copy, 'JP_Sales') 0.04
#outlier2(vgsales_copy, 'Other_Sales') 0.04
Global_Sales_IQR = outlier2(vgsales_copy, 'Global_Sales') #0.42


# In[143]:


# ì •ê·œí™”
# ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì˜ ë²”ìœ„ê°€ ê·¸ë ‡ê²Œ í¬ì§€ì•Šê¸° ë•Œë¬¸ì— ì •ê·œí™”ê°€ í•„ìš”ì—†ì–´ë³´ì´ì§€ë§Œ ì¼ë‹¨ í•œë‹¤
from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()
vgsales_copy = scaler.fit_transform


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:




